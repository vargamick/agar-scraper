# 3DN Scraper - Production Environment Configuration
# Copy this file to .env in the project root and update values

# =============================================================================
# API CONFIGURATION
# =============================================================================
API_HOST=0.0.0.0
API_PORT=3010
API_PREFIX=/api/scraper
ENVIRONMENT=production
DEBUG=false

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
DATABASE_URL=postgresql://scraper_user:CHANGE_THIS_PASSWORD@postgres:5432/scraper_db
DATABASE_ECHO=false

# Connection pool settings for production
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
DB_POOL_RECYCLE=3600

# =============================================================================
# AUTHENTICATION CONFIGURATION
# =============================================================================
# IMPORTANT: Generate a secure secret key using: python deployment/scripts/generate_secret.py
SECRET_KEY=CHANGE_THIS_TO_RANDOM_64_CHAR_STRING_GENERATED_BY_SCRIPT
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60
REFRESH_TOKEN_EXPIRE_DAYS=7

# =============================================================================
# REDIS & CELERY CONFIGURATION
# =============================================================================
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
CELERY_TASK_TIME_LIMIT=7200
CELERY_WORKER_MAX_TASKS_PER_CHILD=100

# =============================================================================
# CLIENT CONFIGURATION
# =============================================================================
ACTIVE_CLIENT=agar

# =============================================================================
# CRAWL4AI CONFIGURATION
# =============================================================================
CRAWL4AI_API_BASE_URL=http://crawl4ai:11235

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
STORAGE_BASE_PATH=/app/scraper_data
STORAGE_JOBS_PATH=/app/scraper_data/jobs
STORAGE_EXPORTS_PATH=/app/scraper_data/exports

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=/app/logs/scraper.log
LOG_ROTATION=100 MB
LOG_RETENTION=30 days

# =============================================================================
# SCRAPER CONFIGURATION
# =============================================================================
SCRAPER_DEFAULT_TIMEOUT=60
SCRAPER_DEFAULT_USER_AGENT="3DN-Scraper/1.0"
SCRAPER_RESPECT_ROBOTS_TXT=true
SCRAPER_MAX_DEPTH=5
SCRAPER_MAX_PAGES_PER_JOB=5000

# =============================================================================
# AWS S3 CONFIGURATION
# =============================================================================
# Enable S3 upload integration
S3_ENABLED=true

# S3 Bucket Configuration
S3_BUCKET_NAME=agar-documentation
S3_REGION=ap-southeast-2

# AWS Credentials - REPLACE WITH YOUR CREDENTIALS
# Use IAM user with minimal S3 permissions
S3_ACCESS_KEY_ID=YOUR_AWS_ACCESS_KEY_ID
S3_SECRET_ACCESS_KEY=YOUR_AWS_SECRET_ACCESS_KEY

# S3 Upload Settings
S3_UPLOAD_ON_COMPLETION=true
S3_PREFIX=agar/
S3_UPLOAD_TIMEOUT=300
S3_MAX_RETRIES=3

# =============================================================================
# CORS CONFIGURATION
# =============================================================================
# Add your domain(s) here
CORS_ORIGINS=https://askagar.com,https://www.askagar.com,http://localhost:8080

# =============================================================================
# POSTGRESQL CONFIGURATION (for docker-compose)
# =============================================================================
POSTGRES_USER=scraper_user
POSTGRES_PASSWORD=CHANGE_THIS_PASSWORD
POSTGRES_DB=scraper_db
