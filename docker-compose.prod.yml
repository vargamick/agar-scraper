# 3DN Scraper - Production Docker Compose Configuration
# Optimized for Coolify deployment

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-scraper_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB:-scraper_db}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-scraper_user} -d ${POSTGRES_DB:-scraper_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  # Redis (for job queue and caching)
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  # Crawl4AI Service - Web scraping engine
  crawl4ai:
    image: unclecode/crawl4ai:latest
    environment:
      - PYTHONUNBUFFERED=1
    shm_size: 2g
    restart: always
    networks:
      - scraper-network
    volumes:
      - crawl4ai-cache:/root/.crawl4ai

  # Scraper API Service
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    expose:
      - "3010"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Database
      - DATABASE_URL=postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-scraper_db}
      - DATABASE_ECHO=false
      - DB_POOL_SIZE=${DB_POOL_SIZE:-20}
      - DB_MAX_OVERFLOW=${DB_MAX_OVERFLOW:-40}
      # Redis
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      # API
      - API_HOST=0.0.0.0
      - API_PORT=3010
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - DEBUG=false
      # API Key auth
      - API_KEY=${API_KEY:-}
      # Crawl4AI
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # AWS S3
      - S3_ENABLED=${S3_ENABLED:-true}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_REGION=${S3_REGION:-ap-southeast-2}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - S3_UPLOAD_ON_COMPLETION=${S3_UPLOAD_ON_COMPLETION:-true}
      - S3_PREFIX=${S3_PREFIX:-agar/}
      # Auth
      - SECRET_KEY=${SECRET_KEY}
      - ACCESS_TOKEN_EXPIRE_MINUTES=${ACCESS_TOKEN_EXPIRE_MINUTES:-60}
      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS}
    volumes:
      - scraper-data:/app/scraper_data
      - scraper-logs:/app/logs
    networks:
      - scraper-network
    command: ["bash", "/app/docker/start.sh"]
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3010/api/scraper/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Celery Worker (for background jobs)
  celery-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-scraper_db}
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # AWS S3
      - S3_ENABLED=${S3_ENABLED:-true}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_REGION=${S3_REGION:-ap-southeast-2}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - S3_UPLOAD_ON_COMPLETION=${S3_UPLOAD_ON_COMPLETION:-true}
      - S3_PREFIX=${S3_PREFIX:-agar/}
    volumes:
      - scraper-data:/app/scraper_data
      - scraper-logs:/app/logs
    networks:
      - scraper-network
    command: ["celery", "-A", "api.jobs.celery_app", "worker", "--loglevel=info", "-Q", "celery,scraper,webhooks", "--concurrency=4"]
    restart: always

networks:
  scraper-network:
    driver: bridge

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  crawl4ai-cache:
    driver: local
  scraper-data:
    driver: local
  scraper-logs:
    driver: local
