# 3DN Scraper - Production Docker Compose Configuration
# For deployment to AWS LightSail or other production environments

version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: scraper-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-scraper_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB:-scraper_db}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-scraper_user} -d ${POSTGRES_DB:-scraper_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # Redis (for job queue and caching)
  redis:
    image: redis:7-alpine
    container_name: scraper-redis
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # Crawl4AI Service - Web scraping engine
  crawl4ai:
    image: unclecode/crawl4ai:latest
    container_name: crawl4ai-service
    environment:
      - PYTHONUNBUFFERED=1
    shm_size: 2g
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11235/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - scraper-network
    volumes:
      - crawl4ai-cache:/root/.crawl4ai
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  # Scraper API Service
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: scraper-api
    ports:
      - "127.0.0.1:3010:3010"  # Only expose to localhost (nginx proxy)
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      crawl4ai:
        condition: service_healthy
    environment:
      # Database
      - DATABASE_URL=postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-scraper_db}
      - DATABASE_ECHO=false
      - DB_POOL_SIZE=${DB_POOL_SIZE:-20}
      - DB_MAX_OVERFLOW=${DB_MAX_OVERFLOW:-40}
      # Redis
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      # API
      - API_HOST=0.0.0.0
      - API_PORT=3010
      - ENVIRONMENT=production
      - DEBUG=false
      # Crawl4AI
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # AWS S3
      - S3_ENABLED=${S3_ENABLED:-true}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_REGION=${S3_REGION:-ap-southeast-2}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - S3_UPLOAD_ON_COMPLETION=${S3_UPLOAD_ON_COMPLETION:-true}
      - S3_PREFIX=${S3_PREFIX:-agar/}
      # Memento Integration
      - MEMENTO_ENABLED=${MEMENTO_ENABLED:-false}
      - MEMENTO_API_URL=${MEMENTO_API_URL}
      - MEMENTO_API_KEY=${MEMENTO_API_KEY}
      - MEMENTO_DEFAULT_INSTANCE=${MEMENTO_DEFAULT_INSTANCE:-main-knowledge}
      # Auth
      - SECRET_KEY=${SECRET_KEY}
      - ACCESS_TOKEN_EXPIRE_MINUTES=${ACCESS_TOKEN_EXPIRE_MINUTES:-60}
      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS}
    env_file:
      - .env
    volumes:
      - ./scraper_data:/app/scraper_data
      - ./logs:/app/logs
      - ./config:/app/config:ro
    networks:
      - scraper-network
    command: ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "3010", "--workers", "4"]
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3010/api/scraper/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  # Celery Worker (for background jobs)
  celery-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: scraper-celery-worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      crawl4ai:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-scraper_db}
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # AWS S3
      - S3_ENABLED=${S3_ENABLED:-true}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_REGION=${S3_REGION:-ap-southeast-2}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - S3_UPLOAD_ON_COMPLETION=${S3_UPLOAD_ON_COMPLETION:-true}
      - S3_PREFIX=${S3_PREFIX:-agar/}
      # Memento Integration
      - MEMENTO_ENABLED=${MEMENTO_ENABLED:-false}
      - MEMENTO_API_URL=${MEMENTO_API_URL}
      - MEMENTO_API_KEY=${MEMENTO_API_KEY}
      - MEMENTO_DEFAULT_INSTANCE=${MEMENTO_DEFAULT_INSTANCE:-main-knowledge}
    env_file:
      - .env
    volumes:
      - ./scraper_data:/app/scraper_data
      - ./logs:/app/logs
      - ./config:/app/config:ro
    networks:
      - scraper-network
    command: ["celery", "-A", "api.jobs.celery_app", "worker", "--loglevel=info", "-Q", "celery,scraper,webhooks", "--concurrency=4"]
    restart: always
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # UI Server - Serves static web interface
  ui:
    image: nginx:alpine
    container_name: scraper-ui
    ports:
      - "127.0.0.1:8080:80"  # Only expose to localhost (nginx proxy)
    volumes:
      - ./ui:/usr/share/nginx/html:ro
      - ./deployment/nginx/ui-nginx.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - scraper-network
    restart: always
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

networks:
  scraper-network:
    driver: bridge

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  crawl4ai-cache:
    driver: local
