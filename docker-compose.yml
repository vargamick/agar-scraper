# 3DN Scraper - Docker Compose Configuration
# Orchestrates Crawl4AI, PostgreSQL, Redis, API, and Scraper services

version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: scraper-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=scraper_user
      - POSTGRES_PASSWORD=scraper_password
      - POSTGRES_DB=scraper_db
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U scraper_user -d scraper_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Redis (for job queue and caching)
  redis:
    image: redis:7-alpine
    container_name: scraper-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Crawl4AI Service - Web scraping engine
  crawl4ai:
    image: unclecode/crawl4ai:latest
    container_name: crawl4ai-service
    ports:
      - "11235:11235"
    environment:
      - PYTHONUNBUFFERED=1
    shm_size: 1g
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11235/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - scraper-network
    volumes:
      - crawl4ai-cache:/root/.crawl4ai

  # Scraper API Service
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: scraper-api
    ports:
      - "3010:3010"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      crawl4ai:
        condition: service_healthy
    environment:
      # Database
      - DATABASE_URL=postgresql://scraper_user:scraper_password@postgres:5432/scraper_db
      - DATABASE_ECHO=false
      # Redis
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      # API
      - API_HOST=0.0.0.0
      - API_PORT=3010
      - ENVIRONMENT=production
      - DEBUG=false
      # Crawl4AI
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=INFO
      # AWS S3
      - S3_ENABLED=${S3_ENABLED:-false}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_REGION=${S3_REGION:-us-east-1}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - S3_UPLOAD_ON_COMPLETION=${S3_UPLOAD_ON_COMPLETION:-true}
      - S3_PREFIX=${S3_PREFIX:-scraper-outputs/}
    env_file:
      - .env
    volumes:
      - ./scraper_data:/app/scraper_data
      - ./logs:/app/logs
      - ./config:/app/config:ro
    networks:
      - scraper-network
    command: ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "3010"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3010/api/scraper/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Celery Worker (for background jobs)
  celery-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: scraper-celery-worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      crawl4ai:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://scraper_user:scraper_password@postgres:5432/scraper_db
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=INFO
      # AWS S3
      - S3_ENABLED=${S3_ENABLED:-false}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_REGION=${S3_REGION:-us-east-1}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - S3_UPLOAD_ON_COMPLETION=${S3_UPLOAD_ON_COMPLETION:-true}
      - S3_PREFIX=${S3_PREFIX:-scraper-outputs/}
    env_file:
      - .env
    volumes:
      - ./scraper_data:/app/scraper_data
      - ./logs:/app/logs
      - ./config:/app/config:ro
    networks:
      - scraper-network
    command: ["celery", "-A", "api.jobs.celery_app", "worker", "--loglevel=info", "-Q", "celery,scraper,webhooks"]
    restart: unless-stopped

  # Legacy Agar Scraper Service (CLI mode)
  agar-scraper:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: agar-scraper
    depends_on:
      crawl4ai:
        condition: service_healthy
    environment:
      - CRAWL4AI_API_BASE_URL=http://crawl4ai:11235
      - ACTIVE_CLIENT=agar
      - LOG_LEVEL=INFO
    env_file:
      - .env
    volumes:
      - ./agar_scrapes:/app/agar_scrapes
      - ./logs:/app/logs
      - ./config:/app/config:ro
    networks:
      - scraper-network
    # Uncomment to run automatically
    # command: ["python", "main.py", "--client", "agar", "--full"]
    profiles:
      - cli  # Only start with: docker-compose --profile cli up

networks:
  scraper-network:
    driver: bridge

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  crawl4ai-cache:
    driver: local
