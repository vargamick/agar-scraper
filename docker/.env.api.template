# ==================== 3DN Scraper API Configuration ====================
# Copy this file to .env and fill in your values

# ==================== Application Settings ====================
APP_NAME="3DN Scraper API"
APP_VERSION="1.0.0"
ENVIRONMENT=development  # development, staging, production
DEBUG=true

# API Server
API_HOST=0.0.0.0
API_PORT=3010
API_PREFIX=/api/scraper

# CORS Settings (comma-separated origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ==================== Database Settings ====================

# PostgreSQL (recommended for production)
# DATABASE_URL=postgresql://scraper_user:your_password@localhost:5432/scraper_db

# SQLite (good for development)
DATABASE_URL=sqlite:///./scraper_api.db

# Show SQL queries in logs (set to false in production)
DATABASE_ECHO=false

# Connection pool settings (PostgreSQL only)
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_RECYCLE=3600

# ==================== Authentication Settings ====================

# IMPORTANT: Generate a secure secret key for production!
# Use: python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY=your-super-secret-key-change-this-in-production

# JWT Settings
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60
REFRESH_TOKEN_EXPIRE_DAYS=7

# Password Settings
PASSWORD_MIN_LENGTH=8

# ==================== Redis Settings ====================

REDIS_URL=redis://localhost:6379/0
# REDIS_PASSWORD=your_redis_password  # Uncomment if Redis requires authentication
REDIS_MAX_CONNECTIONS=50

# ==================== Celery Settings (Job Queue) ====================

CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
CELERY_TASK_TIME_LIMIT=7200
CELERY_WORKER_MAX_TASKS_PER_CHILD=100

# ==================== Job Settings ====================

JOB_DEFAULT_TIMEOUT=3600
JOB_MAX_RETRIES=3
JOB_RETRY_DELAY=60

# Rate limiting
JOB_RATE_LIMIT_REQUESTS=10
JOB_RATE_LIMIT_PERIOD=1

# Pagination
MAX_PAGE_SIZE=100
DEFAULT_PAGE_SIZE=20

# ==================== Storage Settings ====================

STORAGE_BASE_PATH=./scraper_data
STORAGE_JOBS_PATH=./scraper_data/jobs
STORAGE_EXPORTS_PATH=./scraper_data/exports

# ==================== Webhook Settings ====================

WEBHOOK_TIMEOUT=30
WEBHOOK_MAX_RETRIES=3
WEBHOOK_RETRY_DELAY=60

# ==================== Memento Integration ====================

MEMENTO_ENABLED=false
MEMENTO_API_URL=http://localhost:3002
MEMENTO_API_KEY=your_memento_api_key
MEMENTO_DEFAULT_INSTANCE=main-knowledge

# Memento processing options
MEMENTO_ENABLE_CHUNKING=true
MEMENTO_ENABLE_EMBEDDING=true
MEMENTO_CHUNK_SIZE=1000
MEMENTO_CHUNK_OVERLAP=200

# ==================== Scraper Settings ====================

SCRAPER_DEFAULT_TIMEOUT=60
SCRAPER_DEFAULT_USER_AGENT="3DN-Scraper/1.0"
SCRAPER_RESPECT_ROBOTS_TXT=true
SCRAPER_MAX_DEPTH=5
SCRAPER_MAX_PAGES_PER_JOB=1000

# ==================== Logging Settings ====================

LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=./logs/api.log
LOG_ROTATION="100 MB"
LOG_RETENTION="30 days"

# ==================== Monitoring Settings ====================

ENABLE_METRICS=false
METRICS_PORT=9090

# ==================== Existing Scraper Settings ====================
# Keep these for compatibility with the existing scraper

ACTIVE_CLIENT=agar

# Crawl4AI settings (for Docker deployment)
CRAWL4AI_API_URL=http://localhost:11235
CRAWL4AI_API_TOKEN=

# Optional overrides
# OUTPUT_DIR=./custom_output
# LOG_LEVEL_SCRAPER=INFO
# RATE_LIMIT_MIN=1
# RATE_LIMIT_MAX=3
# PROXY_URL=http://proxy:8080
