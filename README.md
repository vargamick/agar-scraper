# 3DN Scraper Template - Multi-Client Web Scraping Framework

**Version:** 1.0.0 | **Status:** Production Ready

A production-ready, client-agnostic web scraping framework designed for deploying and managing multiple client scraping projects. Built with modular architecture and configuration-driven design for maximum reusability and maintainability.

---

## ğŸ¯ Overview

The **3DN Scraper Template** is a sophisticated web scraping framework that enables rapid deployment of scraping projects for multiple clients. Originally developed for Agar Cleaning Supplies, it has been refactored into a flexible template that can be adapted to any e-commerce or product catalog website.

### Key Principles

- **Client-Agnostic Core**: Core modules work universally across all client deployments
- **Configuration-Driven**: Client-specific logic contained in configuration files
- **Strategy Pattern**: CSS selectors and extraction logic separated by client
- **Production Ready**: Battle-tested with comprehensive error handling and logging
- **Easy Deployment**: Deploy new clients in minutes with automation scripts
- **ğŸ¯ Fully Dynamic Scraping**: Zero hardcoded data - all categories and products discovered from websites at runtime

> **Architecture Note:** The scraper uses fully dynamic discovery with no hardcoded category lists or fallback values. Categories are automatically scraped from websites, ensuring complete coverage and adaptability to website changes. See [Dynamic Scraping Architecture](docs/dynamic-scraping-architecture.md) for details.

---

## âœ¨ Features

### Multi-Client Architecture
- ğŸ¢ **Multiple Client Support**: Manage unlimited client deployments from single codebase
- ğŸ”§ **Configuration Loader**: Dynamic client configuration loading system
- ğŸ“‹ **Client Templates**: Pre-configured templates for rapid deployment
- ğŸ”„ **Hot-Swappable**: Switch between clients without code changes

### Scraping Capabilities
- ğŸ—‚ï¸ **Hierarchical Categories**: Automatic subcategory detection and recursive scraping
- ğŸ“¦ **Product Details**: Comprehensive product data extraction (name, images, descriptions, SKUs)
- ğŸ“„ **Document Handling**: PDF extraction and download (SDS, PDS, datasheets)
- ğŸ“¸ **Screenshot Capture**: Page screenshots for verification and debugging
- ğŸ§ª **Test Mode**: Limited scraping for testing and development

### Operational Features
- âš¡ **Modular Design**: Each component can run standalone or as part of workflow
- ğŸ“ **Run Management**: Timestamped directories for organized output
- ğŸ”„ **Resume Capability**: Checkpoint system for interrupted runs
- ğŸ“Š **Comprehensive Reporting**: Detailed statistics and download tracking
- ğŸ›¡ï¸ **Error Resilience**: Automatic retries with exponential backoff

---

## ğŸš€ Quick Start

### Option 1: Docker Deployment (Recommended)

**Quick Start with Docker:**

```bash
# 1. Create environment file
cp docker/.env.template .env

# 2. Start services (pulls Crawl4AI image, builds scraper)
docker-compose up -d

# 3. Run test scrape
docker-compose run --rm agar-scraper python main.py --client agar --test

# 4. Check results
ls -la agar_scrapes/
```

**ğŸ“˜ See [Docker Quick Start Guide](docs/quickstart/DOCKER_QUICKSTART.md) for full instructions**

### Option 2: Local Installation

**Prerequisites:**

```bash
# Python 3.11+ required
python --version

# Install dependencies
pip install crawl4ai aiohttp
```

**Installation:**

```bash
# Clone or download the repository
git clone <repository-url>
cd 3dn-scraper-template

# Verify installation
python main.py --help
```

**First Run (Agar Example):**

```bash
# Test mode (limited scraping)
python main.py --client agar --test

# Full scraping run
python main.py --client agar --full
```

---

## ğŸ“š Documentation

Comprehensive documentation is available in the `/docs` directory:

| Document | Description |
|----------|-------------|
| **[Documentation Index](docs/README.md)** | Complete documentation navigation guide |
| **[Client Deployment Guide](docs/quickstart/CLIENT_DEPLOYMENT_GUIDE.md)** | Complete guide for deploying new clients |
| **[Configuration Guide](docs/quickstart/configuration-guide.md)** | Configuration reference and examples |
| **[Extraction Strategies](docs/architecture/extraction-strategies.md)** | CSS selector guide and testing workflows |
| **[API Reference](docs/api/api-reference.md)** | Complete API documentation for all modules |
| **[Architecture Guide](docs/architecture/architecture.md)** | System architecture and design patterns |
| **[Troubleshooting](docs/quickstart/troubleshooting.md)** | Common issues and solutions |

---

## ğŸ—ï¸ Architecture

```
3DN Scraper Template
â”‚
â”œâ”€â”€ config/                      # Configuration system
â”‚   â”œâ”€â”€ base_config.py          # Base configuration class
â”‚   â”œâ”€â”€ config_loader.py        # Dynamic client loader
â”‚   â””â”€â”€ clients/                # Client deployments
â”‚       â”œâ”€â”€ agar/               # Example: Agar client
â”‚       â”‚   â”œâ”€â”€ client_config.py
â”‚       â”‚   â””â”€â”€ extraction_strategies.py
â”‚       â””â”€â”€ example_client/     # Template for new clients
â”‚
â”œâ”€â”€ core/                        # Core scraping modules
â”‚   â”œâ”€â”€ category_scraper.py     # Category discovery
â”‚   â”œâ”€â”€ product_collector.py    # Product URL collection
â”‚   â”œâ”€â”€ product_scraper.py      # Product detail extraction
â”‚   â”œâ”€â”€ product_pdf_scraper.py  # PDF link extraction
â”‚   â”œâ”€â”€ pdf_downloader.py       # PDF file download
â”‚   â””â”€â”€ utils.py                # Shared utilities
â”‚
â”œâ”€â”€ strategies/                  # Strategy interfaces
â”‚   â””â”€â”€ base_strategy.py        # Base extraction strategy
â”‚
â”œâ”€â”€ scripts/                     # Automation scripts
â”‚   â”œâ”€â”€ deploy_new_client.py    # Client deployment wizard
â”‚   â”œâ”€â”€ validate_config.py      # Configuration validation
â”‚   â”œâ”€â”€ test_connection.py      # Connection testing
â”‚   â””â”€â”€ test_extraction.py      # Extraction testing
â”‚
â”œâ”€â”€ docs/                        # Documentation
â””â”€â”€ main.py                      # Main entry point
```

---

## ğŸ’¼ Client Deployment

### Deploy a New Client

Use the automated deployment wizard:

```bash
python scripts/deploy_new_client.py
```

The wizard guides you through:
1. Basic client information (name, URL, categories)
2. CSS selector identification
3. Configuration file generation
4. Extraction strategy creation
5. Validation and testing

### Manual Client Creation

See the [Client Deployment Guide](docs/quickstart/CLIENT_DEPLOYMENT_GUIDE.md) for detailed instructions on:
- Creating client configuration files
- Defining extraction strategies
- Testing and validation
- Deployment best practices

---

## ğŸ® Usage

### Command Line Interface

```bash
# Complete workflow with client selection
python main.py --client <client_name> [--test|--full] [options]

# Examples:
python main.py --client agar --test              # Test mode
python main.py --client agar --full              # Full run
python main.py --client myclient --full -o ./output  # Custom output

# List available clients
python main.py --list-clients

# Validate client configuration
python scripts/validate_config.py agar
```

### Modular Execution

Each core module can run independently:

```bash
# Category discovery
python -m core.category_scraper --client agar -o ./output

# Product collection
python -m core.product_collector --client agar -o ./output

# Product scraping
python -m core.product_scraper --client agar --url <product_url>

# PDF extraction
python -m core.product_pdf_scraper --client agar --products products.json

# PDF download
python -m core.pdf_downloader --run-dir <run_directory>
```

---

## ğŸ“Š Output Structure

```
<client>_scrapes/
â””â”€â”€ <Client>Scrape_20251105_120000/
    â”œâ”€â”€ run_metadata.json          # Run information
    â”œâ”€â”€ categories.json             # All categories
    â”œâ”€â”€ all_products_list.json     # All product URLs
    â”œâ”€â”€ all_products_data.json     # All scraped products
    â”œâ”€â”€ categories/
    â”‚   â””â”€â”€ [category-slug]/
    â”‚       â”œâ”€â”€ subcategories.json  # If hierarchical
    â”‚       â”œâ”€â”€ products_list.json
    â”‚       â””â”€â”€ [subcategory-slug]/ # Nested structure
    â”œâ”€â”€ products/                   # Individual product JSON
    â”œâ”€â”€ pdfs/                       # PDF metadata
    â”œâ”€â”€ PDFs/                       # Downloaded PDFs
    â”‚   â””â”€â”€ [product]/
    â”‚       â”œâ”€â”€ [product]_SDS.pdf
    â”‚       â””â”€â”€ [product]_PDS.pdf
    â”œâ”€â”€ screenshots/                # Page screenshots
    â”œâ”€â”€ logs/                       # Checkpoints
    â””â”€â”€ reports/
        â”œâ”€â”€ final_report.json
        â””â”€â”€ pdf_download_report.json
```

---

## ğŸ”§ Configuration

### Client Configuration Structure

```python
# config/clients/myclient/client_config.py
from config.base_config import BaseConfig

class ClientConfig(BaseConfig):
    CLIENT_NAME = "myclient"
    CLIENT_FULL_NAME = "My Client Company"
    BASE_URL = "https://myclient.com"
    
    CATEGORY_URL_PATTERN = f"{BASE_URL}/category/{{slug}}/"
    PRODUCT_URL_PATTERN = f"{BASE_URL}/product/{{slug}}/"
    
    OUTPUT_PREFIX = "myclient"
    
    # Categories are discovered dynamically from the website
    # No manual category lists needed
```

### Extraction Strategies

```python
# config/clients/myclient/extraction_strategies.py
class MyclientExtractionStrategy:
    @staticmethod
    def get_product_detail_schema():
        return {
            "name": "Product Details",
            "baseSelector": "body",
            "fields": [
                {"name": "product_name", "selector": "h1.title", "type": "text"},
                {"name": "main_image", "selector": "img.main", "type": "attribute", "attribute": "src"},
                # ... more fields
            ]
        }
```

See [Configuration Guide](docs/quickstart/configuration-guide.md) for complete details.

---

## ğŸ“¦ Example: Agar Client

The Agar Cleaning Supplies deployment serves as a reference implementation:

```bash
# Run Agar scraper
python main.py --client agar --full

# Test Agar extraction
python scripts/test_extraction.py agar

# Validate Agar configuration
python scripts/validate_config.py agar
```

### Agar Features

- **Products**: 500+ cleaning products
- **Categories**: 50+ categories with 3-level hierarchy
- **Documents**: SDS and PDS PDF extraction and download
- **Images**: Product screenshots for verification
- **Validation**: 24/24 configuration checks passing

---

## ğŸ› ï¸ Development

### Adding Custom Features

1. **Custom Extraction Logic**: Override methods in extraction strategy class
2. **Additional Fields**: Extend product schema in client configuration
3. **Custom Processing**: Add methods to core modules (maintain backward compatibility)
4. **Client-Specific Tools**: Create in `config/clients/<client>/` directory

### Testing

```bash
# Test client connection
python scripts/test_connection.py <client_name>

# Test extraction strategies
python scripts/test_extraction.py <client_name>

# Validate configuration
python scripts/validate_config.py <client_name>

# Run in test mode
python main.py --client <client_name> --test
```

---

## ğŸ“ˆ Performance

- **Rate Limiting**: Configurable delays between requests (2-5 seconds default)
- **Concurrent Processing**: Async operations where applicable
- **Cache Management**: Bypass caching for fresh data
- **Timeout Handling**: Configurable timeouts per page type
- **Error Recovery**: Automatic retries with exponential backoff
- **Resource Management**: Proper cleanup and connection pooling

---

## ğŸš¨ Error Handling

- âœ… Automatic retries with different extraction strategies
- âœ… Checkpoint saving for resume capability
- âœ… Detailed error logging with context
- âœ… Graceful interruption handling (Ctrl+C)
- âœ… Configuration validation before execution
- âœ… Connection testing before scraping

---

## ğŸ” Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| No products found | Site structure changed - update extraction strategies |
| Timeout errors | Increase PAGE_TIMEOUT in client config |
| Rate limiting | Adjust RATE_LIMIT_MIN/MAX in client config |
| PDF download failures | Check network, increase retries/timeout |
| Configuration errors | Run `validate_config.py` for detailed diagnostics |

See [Troubleshooting Guide](docs/quickstart/troubleshooting.md) for comprehensive solutions.

---

## ğŸ“‹ Requirements

- Python 3.11+
- crawl4ai library
- aiohttp library
- asyncio support

```bash
pip install crawl4ai aiohttp
```

---

## ğŸ¤ Contributing

### Client Contributions

To share a new client deployment:
1. Create client configuration and strategies
2. Test thoroughly with validation scripts
3. Document any client-specific quirks
4. Submit with example run output

### Core Framework Improvements

- Maintain backward compatibility
- Add unit tests for new features
- Update relevant documentation
- Follow existing code style

---

## ğŸ“ Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-11-05 | Multi-client template release |
| 0.9.0 | 2025-10-31 | Agar scraper refactoring |
| 0.5.0 | 2025-10-15 | Initial Agar scraper |

---

## ğŸ”— Related Projects

- [Crawl4AI](https://github.com/unclecode/crawl4ai) - Core scraping engine
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) - HTML parsing
- [aiohttp](https://docs.aiohttp.org/) - Async HTTP client

---

## ğŸ“„ License

For internal use only. Respect website terms of service and robots.txt when deploying to new clients.

---

## ğŸ™ Acknowledgments

- Built with [Crawl4AI](https://github.com/unclecode/crawl4ai)
- Inspired by modular scraping architectures
- Developed and tested with Agar Cleaning Supplies deployment

---

## ğŸ“ Support

- ğŸ“– [Documentation Index](docs/README.md)
- ğŸ› [Troubleshooting Guide](docs/quickstart/troubleshooting.md)
- ğŸ’¬ [API Reference](docs/api/api-reference.md)
- ğŸš€ [Deployment Guide](docs/quickstart/CLIENT_DEPLOYMENT_GUIDE.md)

---

**Happy Scraping! ğŸš€**
